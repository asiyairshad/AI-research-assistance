
ğŸ§  Multimodal Contextual AI Research Assistant (RAG)

A production-oriented Multimodal Retrieval-Augmented Generation (RAG) system that allows users to upload research PDFs and ask grounded questions over both textual content and visual figures (diagrams, images).

This project focuses on correct system design, controlled multimodal reasoning, observability, and engineering trade-offs, rather than just building a demo chatbot.

âœ¨ What This Project Does

ğŸ“„ Accepts one or multiple research PDFs from users

ğŸ” Parses and indexes text content for semantic retrieval

ğŸ–¼ï¸ Extracts images/figures from PDFs for vision-based reasoning

ğŸ§  Uses a decision layer to determine when image understanding is required

ğŸ‘ï¸ Invokes vision models only when necessary (to reduce latency & cost)

ğŸ›¡ï¸ Ensures answers are grounded strictly in retrieved context

ğŸ“Š Provides full observability using LangSmith

âš¡ Runs as an interactive web app using Streamlit

ğŸ—ï¸ High-Level Architecture
PDF Upload
   â”‚
   â”œâ”€â”€ Text Parsing (Docling / PyMuPDF)
   â”‚       â””â”€â”€ Embeddings â†’ ChromaDB (Vector Store)
   â”‚
   â”œâ”€â”€ Image Extraction (PyMuPDF)
   â”‚
User Query
   â”‚
   â”œâ”€â”€ Text Retrieval (Dense Vector Search)
   â”‚
   â”œâ”€â”€ Decision Model (Is vision required?)
   â”‚
   â”œâ”€â”€ Text LLM (Fast path)
   â”‚        OR
   â”‚   Vision LLM (Quality path)
   â”‚
Final Grounded Answer

PDF Upload
   â”‚
   â”œâ”€â”€ Text Parsing (Docling / PyMuPDF)
   â”‚       â””â”€â”€ Embeddings â†’ ChromaDB (Vector Store)
   â”‚
   â”œâ”€â”€ Image Extraction (PyMuPDF)
   â”‚
User Query
   â”‚
   â”œâ”€â”€ Text Retrieval (Dense Vector Search)
   â”‚
   â”œâ”€â”€ Decision Model (Is vision required?)
   â”‚
   â”œâ”€â”€ Text LLM (Fast path)
   â”‚        OR
   â”‚   Vision LLM (Quality path)
   â”‚
Final Grounded Answer


ğŸ§© Key Design Decisions (Why This Matters)
1. Dense Semantic Retrieval

Uses vector embeddings for meaning-based search

Handles paraphrased and conceptual queries better than keyword search

2. Vision-Aware Decision Logic

Images are not always sent to the LLM

A lightweight decision step determines if visual reasoning is required

Prevents unnecessary latency and cost

3. Stateless RAG (No Memory for Now)

Each query is handled independently

Easier evaluation, lower hallucination risk

Industry-standard starting point for RAG systems

4. Observability Over Guesswork

LangSmith traces every step:

1.retrieval
2.prompts
3.decisions
4.latency
5.token usage

ğŸ› ï¸ Tech Stack
Layer	Technology
UI	Streamlit
PDF Parsing	Docling, PyMuPDF
Embeddings	OpenAI
Vector Store	ChromaDB
LLM (Text)	GPT-4o-mini
LLM (Vision)	GPT-4o
Observability	LangSmith
Environment	Python, uv
ğŸ“‚ Project Structure
.
â”œâ”€â”€ app.py              # Streamlit entry point
â”œâ”€â”€ rag.py              # Core RAG + multimodal reasoning
â”œâ”€â”€ ingest.py           # PDF text & image extraction
â”œâ”€â”€ embeddings.py       # Embedding generation
â”œâ”€â”€ vector_store.py     # ChromaDB interface
â”œâ”€â”€ model.py            # Data schemas
â”œâ”€â”€ .env                # API keys (not committed)
â””â”€â”€ README.md

âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone the repository
git clone <repo-url>
cd <project-folder>

2ï¸âƒ£ Create environment & install dependencies
uv venv
uv add streamlit chromadb openai pillow torch transformers python-dotenv langsmith pymupdf

3ï¸âƒ£ Add environment variables (.env)
OPENAI_API_KEY=sk-xxxxxx
LANGCHAIN_API_KEY=ls-xxxxxx
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=multimodal-rag


âš ï¸ Never commit .env to GitHub.

â–¶ï¸ Running the Application
uv run streamlit run app.py


Open in browser:

http://localhost:8501

ğŸ§ª How the System Behaves
Text-only question

â€œExplain multi-head attentionâ€

â¡ï¸ Uses text retrieval + text LLM
â¡ï¸ No images involved

Vision-required question

â€œExplain the architecture shown in the figureâ€

â¡ï¸ System decides vision is required
â¡ï¸ Sends relevant image + context to vision LLM
â¡ï¸ Returns grounded explanation

ğŸ“Š Observability & Debugging (LangSmith)

LangSmith enables:

Full prompt inspection

Retrieval trace analysis

Latency breakdown

Token usage monitoring

Comparison between fast vs quality mode

This allows the system to be measured, not guessed.

â±ï¸ Performance Design

The system supports two inference modes:

Mode	Goal
Fast	Lower latency, lower cost
Quality	Better reasoning, multimodal support

This exposes latency vs quality trade-offs explicitly, which is how real AI systems are engineered.

ğŸ§  What This Project Does NOT Do (Yet)

âŒ Conversational memory

âŒ Long-term document persistence

âŒ Hybrid retrieval (BM25 + dense)

âŒ Automated Recall@K / Precision@K evaluation

These are intentional exclusions to keep the core system clean and verifiable.

ğŸš€ Future Improvements (Planned Evolution)

ğŸ”¹ Add retrieval evaluation (Recall@K, Precision@K)

ğŸ”¹ Introduce hybrid retrieval (dense + sparse)

ğŸ”¹ Image re-ranking for better figure selection

ğŸ”¹ Optional conversational memory (session-based)

ğŸ”¹ Persistent vector storage for multi-user deployments

ğŸ”¹ Deployment on Google Cloud Run

ğŸ‘©â€ğŸ’» Author

Asiya Irshad
B.Tech Computer Science
Interested in Generative AI, Multimodal Systems, and Production RAG Architectures

ğŸ“ Final Note

This project is not a tutorial clone.

It demonstrates:

real RAG architecture

controlled multimodal reasoning

evaluation-ready design

professional observability practices

It is built with the mindset of:

â€œHow would this work in a real AI team?â€

