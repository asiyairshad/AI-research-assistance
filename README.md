ğŸ“˜ Multimodal Research Assistant

A Hybrid Multimodal RAG System with Embedding & Answer Caching

ğŸš€ Overview

This project is a Multimodal Research Assistant that allows users to:

1.Upload research PDFs

2. Ask natural language questions

3. Retrieve grounded answers using semantic search

4. Automatically retrieve and explain relevant figures

5. Switch between fast and high-quality reasoning modes

6. Benefit from embedding and answer caching for low latency

This system combines text retrieval + image retrieval + multimodal LLM reasoning into a single hybrid pipeline.

ğŸ§  What Makes This Different?

This is NOT a basic PDF chatbot.

It implements:

âœ… Hybrid Retrieval (Text + Image Embeddings)
âœ… OpenAI embeddings for semantic text search
âœ… CLIP embeddings for figure retrieval
âœ… In-memory ChromaDB for vector similarity
âœ… SQLite-based embedding caching
âœ… SQLite-based full answer caching
âœ… Vision-enabled GPT model for multimodal reasoning
âœ… Strict context grounding to reduce hallucinations

ğŸ—ï¸ System Architecture
1ï¸âƒ£ Document Ingestion
Extract text chunks from PDF
Extract images from PDF
Track page metadata

2ï¸âƒ£ Dual Embedding Pipeline
Content Type	Embedding Model
Text	OpenAI Embeddings
Images	CLIP (ViT-B/32)
Text and image embeddings are stored in separate Chroma collections to prevent dimension conflicts.

3ï¸âƒ£ Vector Storage
ChromaDB (in-memory)
text_vectors
image_vectors
Used only for similarity search
No disk persistence

4ï¸âƒ£ Caching Layer (SQLite)
To reduce latency:
Text query embeddings cached
Image query embeddings cached
Full LLM answers cached
If a query is repeated:

âœ” No embedding recomputation
âœ” No vector search
âœ” No LLM call
âœ” Near-instant response

5ï¸âƒ£ Query Flow
User submits query
Check answer cache
If exists â†’ return immediately
Retrieve text embeddings (cached if available)
Perform semantic retrieval
If query contains visual keywords:
Retrieve image embeddings (cached)
Retrieve matching figure
Call appropriate model:
gpt-4o-mini (fast mode)
gpt-4o (quality mode)
Save answer to cache

ğŸ”„ Retrieval Strategy
Text retrieval is always performed.
Image retrieval is triggered only if query contains keywords like:
"figure"
"fig"
"diagram"
"visual"
"show"
This avoids unnecessary multimodal calls and reduces latency.

ğŸ› ï¸ Tech Stack
Python
Streamlit
OpenAI API
CLIP (Vision Transformer)
ChromaDB (in-memory)
SQLite (caching layer)
LangSmith (tracing)

Project structure
AI-research-assistance/
â”‚
â”œâ”€â”€ app.py              # Streamlit UI
â”œâ”€â”€ rag.py              # Retrieval + LLM pipeline
â”œâ”€â”€ embeddings.py       # Text & image embeddings
â”œâ”€â”€ vector_store.py     # In-memory Chroma storage
â”œâ”€â”€ cache_store.py      # SQLite caching layer
â”œâ”€â”€ ingest.py           # PDF parsing
â”œâ”€â”€ cache.db            # SQLite database
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Installation
1ï¸âƒ£ Clone Repository
git clone https://github.com/yourusername/multimodal-research-assistant.git
cd multimodal-research-assistant

2ï¸âƒ£ Create Virtual Environment
python -m venv .venv
source .venv/bin/activate      # Mac/Linux
.venv\Scripts\activate         # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Add Environment Variables
Create a .env file:

OPENAI_API_KEY=your_api_key_here
LANGCHAIN_PROJECT="your langchain api key for tracing"

â–¶ï¸ Run the Application
streamlit run app.py


Open:
http://localhost:8501

ğŸ§ª Example Queries
â€œSummarize the paperâ€
â€œExplain masked attentionâ€
â€œExplain Figure 1.1â€
â€œShow the architecture diagramâ€

ğŸ¯ Design Decisions
Why In-Memory Chroma?
Vector search is fast in memory. No need for persistent vector storage for single-session usage.
Why SQLite Caching?
To avoid recomputation:
Reduces latency
Reduces API cost
Prevents redundant embedding calls
Prevents redundant LLM calls
Why Separate Text and Image Collections?
OpenAI and CLIP embeddings have different dimensions.
Keeping them separate prevents vector dimension conflicts.

ğŸš§ Future Improvements
Add TTL-based cache expiration
Add RAG evaluation metrics
Improve image ranking strategy
Add multi-document cross-paper retrieval
Add semantic chunk ranking improvements

ğŸ§  Skills Demonstrated
Multimodal RAG design
Hybrid embedding architecture
Vision-grounded LLM reasoning
Vector database design
Latency optimization via caching
Hallucination mitigation
Production-style pipeline structuring

ğŸ‘©â€ğŸ’» Author

Asiya Irshad
B.Tech CSE | AI & Generative AI Enthusiast

Interested in:
Multimodal AI
Retrieval-Augmented Generation
AI System Design
Generative AI Engineering

â­ If You Found This Useful

Feel free to connect, fork, or contribute!
